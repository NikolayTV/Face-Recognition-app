{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nikolay/.pyenv/versions/3.7.3/envs/main/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models/model-r34-amf/model 0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean \n",
    "from insightface.embedder import InsightfaceEmbedder\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import IPython\n",
    "\n",
    "model_path = \"models/model-r34-amf/model\"\n",
    "embedder = InsightfaceEmbedder(model_path=model_path, epoch_num='0000', image_size=(112, 112),\n",
    "                               no_face_raise=False,\n",
    "                               MTCNN_min_face_size=50, MTCNN_steps_threshold=[0.7, 0.7, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople = pd.DataFrame()\n",
    "dfPeople['name'] = 'object'\n",
    "dfPeople['path'] = 'object'\n",
    "dfPeople['embedding'] = 'object'\n",
    "dfPeople['bbox'] = 'object'\n",
    "\n",
    "faceDetection_imgHeight = 512\n",
    "\n",
    "path = 'data/People/'\n",
    "\n",
    "def get_embeddings(path):\n",
    "    people_list = os.listdir(path)\n",
    "    \n",
    "    count = 0\n",
    "    print(f'Обнаружено {len(people_list)} индивидов')\n",
    "    print(f'Создание базы эмбеддингов')\n",
    "    \n",
    "    for man in tqdm(people_list):\n",
    "        photos = os.listdir(f'data/People/{man}')\n",
    "        for photo in photos:\n",
    "            img_path = f'data/People/{man}/{photo}'\n",
    "            img = cv2.imread(img_path)\n",
    "            emb_img, bboxes_img, points = embedder.embed_image(img, faceDetection_imgHeight=faceDetection_imgHeight)\n",
    "            dfPeople.at[count, 'name'] = man\n",
    "            dfPeople.at[count, 'path'] = img_path\n",
    "            dfPeople.at[count, 'embedding'] = emb_img[0]\n",
    "            dfPeople.at[count, 'bbox']      = bboxes_img[0]\n",
    "            count += 1\n",
    "    return dfPeople\n",
    "\n",
    "dfPeople = get_embeddings(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_baza(dfPeople):\n",
    "    for i in dfPeople.index:\n",
    "        path = dfPeople.at[i, 'path']\n",
    "        img = plt.imread(path)\n",
    "        startPoint = dfPeople.at[i, 'bbox'][:2]\n",
    "        endPoint = dfPeople.at[i, 'bbox'][2:4]\n",
    "        cv2.rectangle(img, tuple(startPoint), tuple(endPoint), (0,255,0), 4)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "### Averagin embedding\n",
    "def average_embeddings(dfPeople):\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = 'name'\n",
    "    df['embedding'] = 'embedding'\n",
    "\n",
    "    for ind, name in enumerate(dfPeople['name'].value_counts().index):\n",
    "        person = dfPeople[dfPeople['name'] == name]\n",
    "        df.at[ind, 'name'] = name\n",
    "        df.at[ind, 'embedding'] = np.average(person['embedding'].values, axis=0)\n",
    "    return df\n",
    "\n",
    "def process_frame(bgr_frame, faceDetection_imgHeight):\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_encodings, face_locations, face_points = embedder.embed_image(bgr_frame, faceDetection_imgHeight=faceDetection_imgHeight)\n",
    "    if face_encodings != None:\n",
    "        face_names = []\n",
    "        dists_list = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            tolerance = 1\n",
    "            distances = np.linalg.norm(dfPeople['embedding'].values.tolist() - face_encoding, axis=1)\n",
    "            matches = distances <= tolerance\n",
    "            min_dist = min(distances)\n",
    "            dists_list.append(min_dist)\n",
    "            name = 'Unknown'\n",
    "            if matches.sum() > 0:\n",
    "                name = dfPeople['name'][matches].value_counts().index[0]\n",
    "            face_names.append(name)\n",
    "\n",
    "        # Label the results\n",
    "        ind = 0\n",
    "        for (left, top, right, bottom), name in zip(face_locations, face_names):\n",
    "            # Draw a box around the face\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(bgr_frame, (left, top), (right, bottom), (0, 255, 0), 4)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(bgr_frame, f\"{name}, {str(dists_list[ind])[:4]}\", (left, bottom), font, 0.7, \n",
    "                        (0, 255, 0), 1)\n",
    "            ind += 1\n",
    "\n",
    "    return bgr_frame\n",
    "\n",
    "#Mobilenet - 21.42 length of the descriptor - 128 -\n",
    "#Resnet34 - 41 length of the descriptor - 512 - \n",
    "#Resnet100 - 81 length of the descriptor - 512 - \n",
    "# show_baza(dfPeople)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-767653110d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgr_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbgr_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceDetection_imgHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ecc7c900c638>\u001b[0m in \u001b[0;36mprocess_frame\u001b[0;34m(bgr_frame, faceDetection_imgHeight)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbgr_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceDetection_imgHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Find all the faces and face encodings in the current frame of video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mface_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbgr_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceDetection_imgHeight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfaceDetection_imgHeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mface_encodings\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mface_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Insightface/insightface/embedder.py\u001b[0m in \u001b[0;36membed_image\u001b[0;34m(self, image, faceDetection_imgHeight)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Insightface/insightface/face_model.py\u001b[0m in \u001b[0;36mget_feature\u001b[0;34m(self, aligned_faces)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "faceDetection_imgHeight=300\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = video_capture.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "while True:\n",
    "    ret, bgr_frame = video_capture.read()\n",
    "    frame = process_frame(bgr_frame, faceDetection_imgHeight)\n",
    "    _, frame = cv2.imencode('.jpg', frame) \n",
    "    i = IPython.display.Image(data=frame)\n",
    "    IPython.display.display(i)\n",
    "    IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def camera_stream(Play, Stop):\n",
    "#     try:\n",
    "#         faceDetection_imgHeight=300 \n",
    "#         if Play:\n",
    "#             video_capture = cv2.VideoCapture(0)\n",
    "        \n",
    "#             ret, frame = video_capture.read()\n",
    "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "#             while True:\n",
    "#                 ret, bgr_frame = video_capture.read()\n",
    "\n",
    "#                 frame = process_frame(bgr_frame, faceDetection_imgHeight)\n",
    "\n",
    "#                 _, frame = cv2.imencode('.jpg', frame) \n",
    "#                 i = IPython.display.Image(data=frame)\n",
    "#                 IPython.display.display(i)\n",
    "#                 IPython.display.clear_output(wait=True)\n",
    "                \n",
    "#                 if Stop:\n",
    "#                     break\n",
    "#                     raise KeyboardInterrupt\n",
    "# #                     video_capture.release()\n",
    "        \n",
    "# #         else:\n",
    "# #             raise KeyboardInterrupt\n",
    "# #             video_capture.release()\n",
    "\n",
    "#     except KeyboardInterrupt:\n",
    "#         video_capture.release()\n",
    "        \n",
    "        \n",
    "# Play = widgets.ToggleButton(\n",
    "#     value=False,\n",
    "#     description='Start',\n",
    "#     disabled=False,\n",
    "#     button_style='',\n",
    "#     tooltip='Start Camera Streaming',\n",
    "#     icon='play')\n",
    "\n",
    "# Stop = widgets.ToggleButton(\n",
    "#     value=False,\n",
    "#     description='Stop',\n",
    "#     disabled=False,\n",
    "#     button_style='',\n",
    "#     tooltip='Stop Streaming',\n",
    "#     icon='stop')\n",
    "        \n",
    "# out = widgets.interactive_output(camera_stream, \n",
    "#                                 {'Play': Play,\n",
    "#                                  'Stop': Stop})\n",
    "\n",
    "# display(VBox([Play, Stop]), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
